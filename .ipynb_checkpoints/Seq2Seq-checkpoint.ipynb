{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f83c6d50081b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/hamil168/Chatbots/blob/master/Seq2Seq.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8HoIuPChD28"
   },
   "source": [
    "### TODO:\n",
    "* refactor data processing (padding, bucketing)\n",
    "* Aggregate CV loss/acc into arrays for early stopping\n",
    "* create config file\n",
    "\n",
    "### Completed:\n",
    "*  Train test split, fixed random seed\n",
    "*  Validation scores to training loop\n",
    "*  Determine CV loss/acc calculations \n",
    "\n",
    "#### distant todo:\n",
    "* beamsearch decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "n4oyS5PcJBxu",
    "outputId": "cb9c8588-193c-4544-a36b-1a436e352139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Chatbots'...\n",
      "remote: Counting objects: 126, done.\u001b[K\n",
      "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
      "remote: Total 126 (delta 70), reused 35 (delta 12), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (126/126), 9.54 MiB | 17.32 MiB/s, done.\n",
      "Resolving deltas: 100% (70/70), done.\n"
     ]
    }
   ],
   "source": [
    "# For a fresh Colab instance, clone fresh:\n",
    "!pip install -q xlrd\n",
    "!git clone https://github.com/hamil168/Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jw1qRtMF32Ke"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nGvvfIY0JCyk",
    "outputId": "71833751-466b-4e26-cd12-fa245f467313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Chatbots\n"
     ]
    }
   ],
   "source": [
    "# Change to Colab directory:\n",
    "cd Chatbots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "D7PTpdU00-eR",
    "outputId": "4e83616c-0b52-46de-bc56-1ef7effd234a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/hamil168/Chatbots\r\n",
      " * branch            master     -> FETCH_HEAD\n",
      "Already up-to-date.\n"
     ]
    }
   ],
   "source": [
    "# For an existing Colab instance, pull from master, uncomment this:\n",
    "\n",
    "!git pull https://github.com/hamil168/Chatbots master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "oWBAHK8kWX26",
    "outputId": "13f54da9-9482-483b-e49c-433e60d6455e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell Movie Script Database EDA.ipynb  Preproc.ipynb  Seq2Seq.ipynb\r\n",
      "movie_conversations.txt                  preproc.py\r\n",
      "movie_lines.txt                          README.md\r\n"
     ]
    }
   ],
   "source": [
    "# Files as they appear in the repo clone\n",
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "IFaioRdhLyFI",
    "outputId": "97a04e80-ac91-4d80-a836-085f893aa8fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.5MB/s \n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.24.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tqdm   ### use later when in .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "86Muxioq5zlF",
    "outputId": "d02490ad-4057-4726-a87b-c9e5cc3e4975"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "from tensorflow.python.layers.core import Dense\n",
    "#from tqdm import tqdm    ### use later when in .py files\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "global graph1, model\n",
    "\n",
    "graph1 = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvAgspuguXRa"
   },
   "outputs": [],
   "source": [
    "#!python preproc.py\n",
    "from preproc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3IZHUqE1hXD"
   },
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mq2OEcQEZMU3"
   },
   "outputs": [],
   "source": [
    "id2l, cid, questions, answers, clean_questions, clean_answers, word2count, sorted_clean_questions, sorted_clean_answers = preproc_steps(lines,conversations)\n",
    "\n",
    "questionswords2int, answerswords2int = map_questions_and_answers_to_integers(word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76YSZQggcUGp"
   },
   "outputs": [],
   "source": [
    "#conversations[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wB9CHeplaNlf"
   },
   "outputs": [],
   "source": [
    "#get_conversations_ids(conversations[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHKO_vKW7sCL"
   },
   "outputs": [],
   "source": [
    "#print(questions[0:3])\n",
    "#print(clean_questions[0:3])\n",
    "#print(sorted_clean_questions[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28kMPJR05Y8m"
   },
   "outputs": [],
   "source": [
    "# Create placeholder for inputs and the targets\n",
    "# in TF, all variables are tensors\n",
    "# need to go from NP --> TF tensors\n",
    "# need placeholders for every TF variables inputs and targets\n",
    "\n",
    "def model_inputs():\n",
    "  #inputs and targets are 2D matrices\n",
    "  inputs = tf.placeholder(tf.int32, [None, None], name = 'inputs') \n",
    "  targets = tf.placeholder(tf.int32, [None, None], name = 'targets')\n",
    "  keep_prob = tf.placeholder(tf.float32, name = 'dropout_rate') #dropout\n",
    "  \n",
    "  #lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "  \n",
    "  encoder_sequence_length = tf.placeholder(tf.int32, (None, ), name='encoder_sequence_length')\n",
    "  decoder_sequence_length = tf.placeholder(tf.int32, (None, ), name='decoder_sequence_length')\n",
    "  max_sequence_length = tf.reduce_max(decoder_sequence_length, name='max_sequence_length')\n",
    "  \n",
    "  return inputs, targets, keep_prob, encoder_sequence_length, decoder_sequence_length, max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcPkYl7184NK"
   },
   "outputs": [],
   "source": [
    "# Create encoder RNN layer\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, \n",
    "                encoder_sequence_length, keep_prob, encoder_embedding_size, encoder_word_count):\n",
    "  \n",
    "  \n",
    "  # LSTM cell class\n",
    "  # rnn_size: number of input tensors\n",
    "  # sequence_length: length of each question in the atch\n",
    "  \n",
    "  \n",
    "  def cell(units, rate):\n",
    "    layer = tf.contrib.rnn.BasicLSTMCell(units)\n",
    "    return tf.contrib.rnn.DropoutWrapper(layer, rate)\n",
    "\n",
    "  encoder_cell_fw = tf.contrib.rnn.MultiRNNCell([cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "  #encoder_cell_bw = tf.contrib.rnn.MultiRNNCell([cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "\n",
    "  encoder_embeddings = tf.contrib.layers.embed_sequence(rnn_inputs, encoder_word_count, encoder_embedding_size)\n",
    "  \n",
    "  # bidirection rnn function (creates dynamic bidirectional network)\n",
    "  # builds independent forward and backward rnn\n",
    "  # need ot make sure the ends match\n",
    "  # (first element is encoder_output)\n",
    "  #encoder_outputs, encoder_states = tf.nn.bidirectional_dynamic_rnn(encoder_cell_fw,\n",
    "  encoder_outputs, encoder_states = tf.nn.dynamic_rnn(encoder_cell_fw,                                                                    \n",
    "                                                   #cell_bw = encoder_cell_bw,\n",
    "                                                   inputs = encoder_embeddings,\n",
    "                                                   sequence_length = encoder_sequence_length,\n",
    "                                                   dtype = tf.float32)\n",
    "  \n",
    "  return encoder_outputs, encoder_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3v5y5YB6lHv"
   },
   "outputs": [],
   "source": [
    "# preprocessing the targets\n",
    "# need batches, \n",
    "# need each to start with <SOS> token\n",
    "\n",
    "def preprocess_decoder_inputs(targets, word2int_dict, batch_size):\n",
    "  \"\"\"\n",
    "\n",
    "  Prepares the decoder inputs (i.e. the 'targets') for use\n",
    "     \n",
    "     Inputs: \n",
    "        targets: the input for the decoder for training.\n",
    "        word2int_dict: one of the dictionaries used to map a word to its integer\n",
    "        batch_size: size of each batch for model training\n",
    "        \n",
    "     Outputs:\n",
    "        preprocessed_targets: the processed version of the decoder inputs\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  # Using <SOS> for \"start of string\", create a tensor with one per batch element\n",
    "  left_side = tf.fill([batch_size, 1], word2int_dict['<SOS>'])\n",
    "    \n",
    "  # Take the targets and remove the last member of each sample (it is blank)\n",
    "  right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "  \n",
    "  # Add the <SOS> to the left side of every target phrase\n",
    "  return tf.concat([left_side, right_side], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrO9rYaxfuUc"
   },
   "source": [
    "### Attention\n",
    "- (warning for later, when I add Beam Search) **NOTE** If you are using the `BeamSearchDecoder` with a cell wrapped in\n",
    "`AttentionWrapper`\n",
    "- will also need to return here with DeviceWrapper for multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Po14wIN5hQLz"
   },
   "outputs": [],
   "source": [
    "def decoder(decoder_inputs, encoder_state, decoder_cell, decoder_embedding_size,\n",
    "            vocabulary_size, decoder_sequence_length, max_sequence_length,\n",
    "            word2id_dict, batch_size):\n",
    "  \n",
    "  \n",
    "  embedding_layer = tf.Variable(tf.random_uniform([vocabulary_size, decoder_embedding_size]))\n",
    "  embeddings = tf.nn.embedding_lookup(embedding_layer, decoder_inputs)\n",
    "  \n",
    "  output_layer = Dense(vocabulary_size, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.1))\n",
    "    \n",
    "  with tf.variable_scope('decoder'):\n",
    "  \n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(embeddings, sequence_length = decoder_sequence_length)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(cell = decoder_cell,\n",
    "                                             helper = train_helper,\n",
    "                                             initial_state = encoder_state, \n",
    "                                                    output_layer = output_layer)\n",
    "\n",
    "\n",
    "    # returns (final_outputs, final_state, final_sequence_lengths)\n",
    "    train_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder = train_decoder,\n",
    "                                                             impute_finished = True,\n",
    "                                                             maximum_iterations = max_sequence_length)\n",
    "    ###########################\n",
    "    #decoder_output_dropout is handled in a attention wrapper function outside of this functinon                                                                \n",
    "                                                   \n",
    "         \n",
    "  with tf.variable_scope('decoder', reuse=True):\n",
    "  \n",
    "    starting_id_vector = tf.tile(tf.constant([word2id_dict['<SOS>']], dtype=tf.int32), [batch_size], name = 'starting_id_vector')                                               \n",
    "                                                   \n",
    "    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_layer, \n",
    "                                                            starting_id_vector,\n",
    "                                                           word2id_dict['<EOS>'])                                                   \n",
    "\n",
    "    infer_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                    helper = infer_helper,\n",
    "                                                    initial_state = encoder_state,\n",
    "                                                    output_layer=output_layer)\n",
    "\n",
    "\n",
    "    # returns (final_outputs, final_state, final_sequence_lengths)\n",
    "    infer_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder,\n",
    "                                                             impute_finished = True,\n",
    "                                                             maximum_iterations = max_sequence_length)\n",
    "    \n",
    "                                                   \n",
    "  return train_decoder_output, infer_decoder_output\n",
    "                                                   \n",
    "                                                   \n",
    "                                              \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZs76eWQcGGl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def attention_mechanism(rnn_size, keep_prob, encoder_outputs, encoder_states, encoder_sequence_length, batch_size):\n",
    "  \n",
    "  \n",
    "  def cell(units, probs):\n",
    "    layer = tf.contrib.rnn.BasicLSTMCell(units)\n",
    "    return tf.contrib.rnn.DropoutWrapper(layer, probs)\n",
    "  \n",
    "  decoder_cell = cell(rnn_size, keep_prob)\n",
    "  \n",
    "  attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, \n",
    "                                                            encoder_outputs,\n",
    "                                                            encoder_sequence_length)\n",
    "  \n",
    "  decoder_cell_wrapped = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,\n",
    "                                                            attention_mechanism,\n",
    "                                                            rnn_size / 2)\n",
    "  \n",
    "  attention_ought = decoder_cell_wrapped.zero_state(batch_size = batch_size, dtype = tf.float32)\n",
    "  \n",
    "  encoder_state_new = attention_ought.clone(cell_state = encoder_states[-1])\n",
    "  \n",
    "  return decoder_cell_wrapped, encoder_state_new\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43aLI3wXIBwc"
   },
   "outputs": [],
   "source": [
    "def optimizer_loss(outputs, targets, decoder_sequence_length, max_sequence_length, learning_rate, clip_rate):\n",
    "    '''\n",
    "\t\n",
    "\t\tFunction used to define optimizer and loss function\n",
    "\t\tInputs:\n",
    "\t\t\toutputs - outputs got from decoder part of the network\n",
    "\t\t\ttargets - expected outputs/ labels\n",
    "\t\t\tdec_seq_len -\n",
    "\t\t\tmax_seq_len - \n",
    "\t\t\tlearning_rate - small nubmer used to decrease value of gradients used to update our network\n",
    "\t\t\tclip_rate - tolerance boundries for clipping gradients\n",
    "\t\tOutputs:\n",
    "\t\t\tloss -\n",
    "\t\t\ttrained_opt - optimizer with clipped gradients\n",
    "    '''\n",
    "    logits = tf.identity(outputs.rnn_output)\n",
    "    \n",
    "    mask_weights = tf.sequence_mask(decoder_sequence_length, max_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('opt_loss'):\n",
    "        #using sequence_loss to optimize the seq2seq model\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits, \n",
    "                                                targets, \n",
    "                                                mask_weights)\n",
    "        \n",
    "        #Define optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        #Next 3 lines used to clip gradients {Prevent gradient explosion problem}\n",
    "        gradients = tf.gradients(loss, tf.trainable_variables())\n",
    "        clipped_grads, _ = tf.clip_by_global_norm(gradients, clip_rate)\n",
    "        trained_opt = optimizer.apply_gradients(zip(clipped_grads, tf.trainable_variables()))\n",
    "        \n",
    "    return loss, trained_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adeYZW4j_7-V"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq_Model(object):\n",
    "    \n",
    "    def __init__(self, learning_rate, batch_size, encoder_embedded_size, decoder_embedded_size, rnn_size, \n",
    "                 number_of_layers, vocab_size, word2id_dict, clip_rate):\n",
    "        \n",
    "        #tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs, self.targets, self.keep_prob, self.encoder_sequence_length, self.decoder_sequence_length, max_sequence_length = model_inputs()\n",
    "        \n",
    "        \n",
    "        enc_outputs, enc_states = encoder_rnn(self.inputs, \n",
    "                                          rnn_size,\n",
    "                                          number_of_layers, \n",
    "                                          self.encoder_sequence_length, \n",
    "                                          self.keep_prob, \n",
    "                                          encoder_embedded_size, \n",
    "                                          vocab_size)\n",
    "        \n",
    "        dec_inputs = preprocess_decoder_inputs(self.targets, \n",
    "                                                  word2id_dict, \n",
    "                                                  batch_size)\n",
    "        \n",
    "        \n",
    "        decoder_cell, encoder_states_new = attention_mechanism(rnn_size, \n",
    "                                                          self.keep_prob, \n",
    "                                                          enc_outputs, \n",
    "                                                          enc_states, \n",
    "                                                          self.encoder_sequence_length, \n",
    "                                                          batch_size)\n",
    "        \n",
    "        train_outputs, inference_output = decoder(dec_inputs, \n",
    "                                                  encoder_states_new, \n",
    "                                                  decoder_cell,\n",
    "                                                  decoder_embedded_size, \n",
    "                                                  vocab_size, \n",
    "                                                  self.decoder_sequence_length, \n",
    "                                                  max_sequence_length, \n",
    "                                                  word2id_dict, \n",
    "                                                  batch_size)\n",
    "        \n",
    "        self.predictions  = tf.identity(inference_output.sample_id, name='preds')\n",
    "        \n",
    "        self.loss, self.opt = optimizer_loss(train_outputs, \n",
    "                                       self.targets, \n",
    "                                       self.decoder_sequence_length, \n",
    "                                       max_sequence_length, \n",
    "                                       learning_rate, \n",
    "                                       clip_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-91sT3evJ-k5"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckHWyDJgqhdm"
   },
   "outputs": [],
   "source": [
    "# Apply padding to the sequences so the question and answer sequences are the same\n",
    "\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "  max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "  return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxVs-CJgYLCO"
   },
   "outputs": [],
   "source": [
    "# Fxn to split data itno batches for batch gradient descent                                            \n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "  padded_questions_in_batch = []\n",
    "  padded_answers_in_batch = []\n",
    "  final_question_batches = []\n",
    "  final_answer_batches = []\n",
    "  \n",
    "  for batch_index in range(0,len(questions) // batch_size):\n",
    "      start_index = batch_index * batch_size\n",
    "                                            \n",
    "      questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "      answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "      \n",
    "                         \n",
    "      final_question_batches.append(questions_in_batch)\n",
    "      final_answer_batches.append(answers_in_batch)\n",
    "       \n",
    "  return final_question_batches, final_answer_batches\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3adoQBBzf9V"
   },
   "outputs": [],
   "source": [
    "# Next up: hyper parameters\n",
    "epochs = 2 #100\n",
    "batch_size = 10  #64 make bigger to make faster\n",
    "rnn_size = 32 # 512\n",
    "num_layers = 1  #3\n",
    "encoding_embedding_size = 32 #512  # 512 col in embedding matrix\n",
    "decoding_embedding_size = 32 #512\n",
    "learning_rate = 0.1 # 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.01 #0.0001\n",
    "keep_prob = 0.5\n",
    "keep_probability = 0.5  # based on hinton paper '14'\n",
    "clip= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZhqWF4lZMZm"
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "\n",
    "TTS_TEST_SIZE = 0.2\n",
    "TTS_SEED = 12345\n",
    "\n",
    "train_Q, val_Q, train_A, val_A = \\\n",
    "        train_test_split(sorted_clean_questions, sorted_clean_answers,\n",
    "                     test_size = TTS_TEST_SIZE, random_state = TTS_SEED)\n",
    "\n",
    "padded_train_Q_batches, padded_train_A_batches = \\\n",
    "             split_into_batches(apply_padding(train_Q,questionswords2int),\n",
    "                                apply_padding(train_A,questionswords2int), \n",
    "                                batch_size)\n",
    "\n",
    "#padded_val_Q = apply_padding(val_Q, questionswords2int)\n",
    "#padded_val_A = apply_padding(val_A, questionswords2int)\n",
    "\n",
    "\n",
    "padded_val_Q_batches, padded_val_A_batches = \\\n",
    "            split_into_batches(apply_padding(val_Q, questionswords2int),\n",
    "                              apply_padding(val_A, questionswords2int),\n",
    "                              batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoddAR_lpdl3"
   },
   "outputs": [],
   "source": [
    "# If needed during testing\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTvlebyhA0YC"
   },
   "outputs": [],
   "source": [
    "# instantiate the Seq2Seq model using graph1\n",
    "# starts with resetting graph1 for debugging purposes\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph1 = tf.get_default_graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "  \n",
    "  model = Seq2Seq_Model(learning_rate, batch_size, encoding_embedding_size, \n",
    "                        decoding_embedding_size,\n",
    "                        rnn_size, num_layers, len(word2count), \n",
    "                        questionswords2int, clip)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-ZW14YCjxNt"
   },
   "outputs": [],
   "source": [
    "# establish session using graph1\n",
    "\n",
    "session = tf.Session(graph = graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "c3dGVvipCPV9",
    "outputId": "69779376-bec4-473b-db4a-6519ff42483e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bucket 0:  | Loss: 11.146756172180176  | Accuracy: 0.5808664259927798\n",
      " Bucket 2:  | Loss: 6.221677780151367  | Accuracy: 0.8487364620938628\n",
      "EPOCH: 0/2 \n",
      " --->| loss: 6.221677780151367 val: 0.6152670383453369 \n",
      " --->| acc: 0.8487364620938628 val: 0.9630434782608696\n",
      " Bucket 0:  | Loss: 0.19514213502407074  | Accuracy: 0.984115523465704\n",
      " Bucket 2:  | Loss: 0.22734414041042328  | Accuracy: 0.9831528279181709\n",
      "EPOCH: 1/2 \n",
      " --->| loss: 3.224510908126831 val: 0.572544515132904 \n",
      " --->| acc: 0.9159446450060169 val: 0.9630434782608696\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "# initialize global variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "PRINT_ERROR = True\n",
    "\n",
    "# empty lists\n",
    "epoch_accuracy = []\n",
    "epoch_loss = []\n",
    "\n",
    "# Start training loop\n",
    "for i in range(epochs):\n",
    "  \n",
    "  # empty lists to collect loss and acc for bucketd training sets\n",
    "  batch_accuracy = []\n",
    "  batch_loss = []\n",
    "  \n",
    "  # for bucket index\n",
    "  # tqdm is a progress bar that does not look good in notebooks\n",
    "  # but looks good in command line\n",
    "  #for b_idx in tqdm(len(padded_train_Q_batches)): ### commented out for debugging\n",
    "  \n",
    "  for b_idx in range(3): #<-- for debugging\n",
    "  \n",
    "    # convert the python arrays to numpy arrays\n",
    "    X_batch = np.asarray(padded_train_Q_batches[b_idx])\n",
    "    y_batch = np.asarray(padded_train_A_batches[b_idx])\n",
    "    ### print(X_batch.shape, y_batch.shape)\n",
    "\n",
    "    # create the feed_dict for the model creation steps\n",
    "    feed_dict = {model.inputs:X_batch, \n",
    "         model.targets:y_batch, \n",
    "         model.keep_prob:keep_prob, \n",
    "         model.decoder_sequence_length:[len(y_batch[0])]*batch_size,\n",
    "         model.encoder_sequence_length:[len(X_batch[0])]*batch_size}\n",
    "\n",
    "    \n",
    "    \n",
    "    # a single step of batch gradient descent\n",
    "    cost, _, preds = session.run([model.loss, model.opt, model.predictions], feed_dict=feed_dict)\n",
    "\n",
    "    # collect loss/acc for each batch\n",
    "    batch_loss.append(cost)\n",
    "    batch_accuracy.append(get_accuracy(y_batch, preds))\n",
    "\n",
    "       \n",
    "    #if(PRINT_ERROR == True and b_idx%100 == 0): \n",
    "    if(PRINT_ERROR == True and b_idx%2 == 0):\n",
    "      print(\" Bucket {}:\".format(b_idx), \n",
    "          \" | Loss: {}\".format(np.mean(batch_loss)), \n",
    "          \" | Accuracy: {}\".format(np.mean(batch_accuracy)))\n",
    "\n",
    "  epoch_loss.append(np.mean(batch_loss))\n",
    "  epoch_accuracy.append(np.mean(batch_accuracy))\n",
    "  \n",
    "  # Print epoch and CV loss/accuracy:\n",
    "  #if(PRINT_ERROR == True and i%100 == 0):\n",
    "  if(PRINT_ERROR == True and i%1 == 0):\n",
    "      val_losses = []\n",
    "      val_acc = []\n",
    "      \n",
    "      for v_idx in range(2):#len(padded_val_Q_batches)):\n",
    "        X_val = np.asarray(padded_val_Q_batches[v_idx])\n",
    "        \n",
    "        y_val = np.asarray(padded_val_A_batches[v_idx])\n",
    "        ### print(X_val.shape, y_val.shape)\n",
    "        \n",
    "        # validation feed_dict\n",
    "        val_feed_dict = {model.inputs:X_val, \n",
    "                         model.targets:y_val, \n",
    "                         model.keep_prob:1, \n",
    "                         model.decoder_sequence_length:[len(y_val[0])]*batch_size,\n",
    "                         model.encoder_sequence_length:[len(X_val[0])]*batch_size}\n",
    "\n",
    "        \n",
    "        # run model loss and predictions, but not optimization -- scoring, not training!\n",
    "        val_loss, val_preds = session.run([model.loss, model.predictions], feed_dict = val_feed_dict)\n",
    "                \n",
    "        val_losses.append(val_loss)\n",
    "        val_acc.append(get_accuracy(y_val, val_preds))\n",
    "        #val_acc.append(tf.metrics.accuracy(y_val, val_preds)) ##<-- causes attribute error in np.mean()\n",
    "           \n",
    "      print(\"EPOCH: {}/{}\".format(i, epochs), \n",
    "          \"\\n --->| loss: {} val: {}\".format(np.mean(epoch_loss), np.mean(val_losses)), \n",
    "          \"\\n --->| acc: {} val: {}\".format(np.mean(epoch_accuracy), np.mean(val_acc)))\n",
    "\n",
    "  #saver.save(session, \"checkpoint/chatbot_{}.ckpt\".format(i))\n",
    "    \n",
    "session.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DL9bjZEknKvv"
   },
   "outputs": [],
   "source": [
    "#early_stopping_check = 0\n",
    "#early_stopping_stop = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpWXnhU3dN1v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Seq2Seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
