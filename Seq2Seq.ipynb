{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/hamil168/Chatbots/blob/master/Seq2Seq.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8HoIuPChD28"
   },
   "source": [
    "### TODO:\n",
    "* Early stopping\n",
    "* Aggregate CV loss/acc into arrays for early stopping\n",
    "* refactor data processing (padding, bucketing)\n",
    "* create config file\n",
    "\n",
    "### Completed:\n",
    "*  Train test split, fixed random seed\n",
    "*  Validation scores to training loop\n",
    "*  Determine CV loss/acc calculations \n",
    "\n",
    "#### distant todo:\n",
    "* beamsearch decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "n4oyS5PcJBxu",
    "outputId": "cb9c8588-193c-4544-a36b-1a436e352139"
   },
   "outputs": [],
   "source": [
    "# For a fresh Colab instance, clone fresh:\n",
    "#!pip install -q xlrd\n",
    "#!git clone https://github.com/hamil168/Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nGvvfIY0JCyk",
    "outputId": "71833751-466b-4e26-cd12-fa245f467313"
   },
   "outputs": [],
   "source": [
    "# Change to Colab directory:\n",
    "#cd Chatbots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "D7PTpdU00-eR",
    "outputId": "4e83616c-0b52-46de-bc56-1ef7effd234a"
   },
   "outputs": [],
   "source": [
    "# For an existing Colab instance, pull from master, uncomment this:\n",
    "\n",
    "#!git pull https://github.com/hamil168/Chatbots master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "oWBAHK8kWX26",
    "outputId": "13f54da9-9482-483b-e49c-433e60d6455e"
   },
   "outputs": [],
   "source": [
    "# Files as they appear in the repo clone\n",
    "#ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "IFaioRdhLyFI",
    "outputId": "97a04e80-ac91-4d80-a836-085f893aa8fe"
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm   ### use later when in .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs needed for the local virtual environment\n",
    "#!pip install pandas\n",
    "#!pip install time\n",
    "#!pip install re\n",
    "#!pip install sklearn\n",
    "#!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "86Muxioq5zlF",
    "outputId": "d02490ad-4057-4726-a87b-c9e5cc3e4975"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_virtual_environments\\tf19_chatbot\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "\n",
    "from tensorflow.python.layers.core import Dense\n",
    "#from tqdm import tqdm    ### use later when in .py files\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "global graph1, model\n",
    "\n",
    "graph1 = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5918008111785651249\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3177234432\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6471722183456670414\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvAgspuguXRa"
   },
   "outputs": [],
   "source": [
    "#my preproc.py\n",
    "from preproc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3IZHUqE1hXD"
   },
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mq2OEcQEZMU3"
   },
   "outputs": [],
   "source": [
    "id2l, cid, questions, answers, clean_questions, clean_answers, word2count, sorted_clean_questions, sorted_clean_answers = preproc_steps(lines,conversations)\n",
    "\n",
    "questionswords2int, answerswords2int, tokens = map_questions_and_answers_to_integers(word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHKO_vKW7sCL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_sca = max([len(s) for s in sorted_clean_answers])\\nprint(max_sca)\\nmax_idx = [i for i in range(len(sorted_clean_answers)) if len(sorted_clean_answers[i]) == max_sca]\\nprint(sorted_clean_answers[max_idx[0]]) \\nprint(len(answers), len(sorted_clean_answers))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPROC LENGTH LIMITER CHECK:          ######### NOTE TO SELF: Do we go to length 25 what? <EOS consideration>\n",
    "# this should be snipped at len() = MAX_LENGTH (25 for testing); it is 550+ otherwise\n",
    "\"\"\"max_sca = max([len(s) for s in sorted_clean_answers])\n",
    "print(max_sca)\n",
    "max_idx = [i for i in range(len(sorted_clean_answers)) if len(sorted_clean_answers[i]) == max_sca]\n",
    "print(sorted_clean_answers[max_idx[0]]) \n",
    "print(len(answers), len(sorted_clean_answers))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28kMPJR05Y8m"
   },
   "outputs": [],
   "source": [
    "# Create placeholder for inputs and the targets\n",
    "# in TF, all variables are tensors\n",
    "# need to go from NP --> TF tensors\n",
    "# need placeholders for every TF variables inputs and targets\n",
    "\n",
    "def model_inputs():\n",
    "  #inputs and targets are 2D matrices\n",
    "  inputs = tf.placeholder(tf.int32, [None, None], name = 'inputs') \n",
    "  targets = tf.placeholder(tf.int32, [None, None], name = 'targets')\n",
    "  keep_prob = tf.placeholder(tf.float32, name = 'dropout_rate') #dropout\n",
    "  \n",
    "  #lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "  \n",
    "  encoder_sequence_length = tf.placeholder(tf.int32, (None, ), name='encoder_sequence_length')\n",
    "  decoder_sequence_length = tf.placeholder(tf.int32, (None, ), name='decoder_sequence_length')\n",
    "  max_sequence_length = tf.reduce_max(decoder_sequence_length, name='max_sequence_length')\n",
    "  \n",
    "  return inputs, targets, keep_prob, encoder_sequence_length, decoder_sequence_length, max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcPkYl7184NK"
   },
   "outputs": [],
   "source": [
    "# Create encoder RNN layer\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, \n",
    "                encoder_sequence_length, keep_prob, encoder_embedding_size, encoder_word_count):\n",
    "  \n",
    "  \n",
    "  # LSTM cell class\n",
    "  # rnn_size: number of input tensors\n",
    "  # sequence_length: length of each question in the atch\n",
    "  \n",
    "  \n",
    "  def cell(units, rate):\n",
    "    layer = tf.contrib.rnn.BasicLSTMCell(units)\n",
    "    return tf.contrib.rnn.DropoutWrapper(layer, rate)\n",
    "\n",
    "  encoder_cell_fw = tf.contrib.rnn.MultiRNNCell([cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "  #encoder_cell_bw = tf.contrib.rnn.MultiRNNCell([cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "\n",
    "  encoder_embeddings = tf.contrib.layers.embed_sequence(rnn_inputs, encoder_word_count, encoder_embedding_size)\n",
    "  \n",
    "  # bidirection rnn function (creates dynamic bidirectional network)\n",
    "  # builds independent forward and backward rnn\n",
    "  # need ot make sure the ends match\n",
    "  # (first element is encoder_output)\n",
    "  #encoder_outputs, encoder_states = tf.nn.bidirectional_dynamic_rnn(encoder_cell_fw,\n",
    "  encoder_outputs, encoder_states = tf.nn.dynamic_rnn(encoder_cell_fw,                                                                    \n",
    "                                                   #cell_bw = encoder_cell_bw,\n",
    "                                                   inputs = encoder_embeddings,\n",
    "                                                   sequence_length = encoder_sequence_length,\n",
    "                                                   dtype = tf.float32)\n",
    "  \n",
    "  return encoder_outputs, encoder_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3v5y5YB6lHv"
   },
   "outputs": [],
   "source": [
    "# preprocessing the targets\n",
    "# need batches, \n",
    "# need each to start with <SOS> token\n",
    "\n",
    "def preprocess_decoder_inputs(targets, word2int_dict, batch_size):\n",
    "  \"\"\"\n",
    "\n",
    "  Prepares the decoder inputs (i.e. the 'targets') for use\n",
    "     \n",
    "     Inputs: \n",
    "        targets: the input for the decoder for training.\n",
    "        word2int_dict: one of the dictionaries used to map a word to its integer\n",
    "        batch_size: size of each batch for model training\n",
    "        \n",
    "     Outputs:\n",
    "        preprocessed_targets: the processed version of the decoder inputs\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  # Using <SOS> for \"start of string\", create a tensor with one per batch element\n",
    "  left_side = tf.fill([batch_size, 1], word2int_dict['<SOS>'])\n",
    "    \n",
    "  # Take the targets and remove the last member of each sample (it is blank)\n",
    "  right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "  \n",
    "  # Add the <SOS> to the left side of every target phrase\n",
    "  return tf.concat([left_side, right_side], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrO9rYaxfuUc"
   },
   "source": [
    "### Attention\n",
    "- (warning for later, when I add Beam Search) **NOTE** If you are using the `BeamSearchDecoder` with a cell wrapped in\n",
    "`AttentionWrapper`\n",
    "- will also need to return here with DeviceWrapper for multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Po14wIN5hQLz"
   },
   "outputs": [],
   "source": [
    "def decoder(decoder_inputs, encoder_state, decoder_cell, decoder_embedding_size,\n",
    "            vocabulary_size, decoder_sequence_length, max_sequence_length,\n",
    "            word2id_dict, batch_size):\n",
    "  \n",
    "  \n",
    "  embedding_layer = tf.Variable(tf.random_uniform([vocabulary_size, decoder_embedding_size]))\n",
    "  embeddings = tf.nn.embedding_lookup(embedding_layer, decoder_inputs)\n",
    "  \n",
    "  output_layer = Dense(vocabulary_size, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.1))\n",
    "    \n",
    "  with tf.variable_scope('decoder'):\n",
    "  \n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(embeddings, sequence_length = decoder_sequence_length)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(cell = decoder_cell,\n",
    "                                             helper = train_helper,\n",
    "                                             initial_state = encoder_state, \n",
    "                                                    output_layer = output_layer)\n",
    "\n",
    "\n",
    "    # returns (final_outputs, final_state, final_sequence_lengths)\n",
    "    train_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder = train_decoder,\n",
    "                                                             impute_finished = True,\n",
    "                                                             maximum_iterations = max_sequence_length)\n",
    "    ###########################\n",
    "    #decoder_output_dropout is handled in a attention wrapper function outside of this functinon                                                                \n",
    "                                                   \n",
    "         \n",
    "  with tf.variable_scope('decoder', reuse=True):\n",
    "  \n",
    "    starting_id_vector = tf.tile(tf.constant([word2id_dict['<SOS>']], dtype=tf.int32), [batch_size], name = 'starting_id_vector')                                               \n",
    "                                                   \n",
    "    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_layer, \n",
    "                                                            starting_id_vector,\n",
    "                                                           word2id_dict['<EOS>'])                                                   \n",
    "\n",
    "    infer_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                    helper = infer_helper,\n",
    "                                                    initial_state = encoder_state,\n",
    "                                                    output_layer=output_layer)\n",
    "\n",
    "\n",
    "    # returns (final_outputs, final_state, final_sequence_lengths)\n",
    "    infer_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder,\n",
    "                                                             impute_finished = True,\n",
    "                                                             maximum_iterations = max_sequence_length)\n",
    "    \n",
    "                                                   \n",
    "  return train_decoder_output, infer_decoder_output\n",
    "                                                   \n",
    "                                                   \n",
    "                                              \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZs76eWQcGGl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def attention_mechanism(rnn_size, keep_prob, encoder_outputs, encoder_states, encoder_sequence_length, batch_size):\n",
    "  \n",
    "  \n",
    "  def cell(units, probs):\n",
    "    layer = tf.contrib.rnn.BasicLSTMCell(units)\n",
    "    return tf.contrib.rnn.DropoutWrapper(layer, probs)\n",
    "  \n",
    "  decoder_cell = cell(rnn_size, keep_prob)\n",
    "  \n",
    "  attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, \n",
    "                                                            encoder_outputs,\n",
    "                                                            encoder_sequence_length)\n",
    "  \n",
    "  decoder_cell_wrapped = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,\n",
    "                                                            attention_mechanism,\n",
    "                                                            rnn_size / 2)\n",
    "  \n",
    "  attention_ought = decoder_cell_wrapped.zero_state(batch_size = batch_size, dtype = tf.float32)\n",
    "  \n",
    "  encoder_state_new = attention_ought.clone(cell_state = encoder_states[-1])\n",
    "  \n",
    "  return decoder_cell_wrapped, encoder_state_new\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43aLI3wXIBwc"
   },
   "outputs": [],
   "source": [
    "def optimizer_loss(outputs, targets, decoder_sequence_length, max_sequence_length, learning_rate, clip_rate):\n",
    "    '''\n",
    "\t\n",
    "\t\tFunction used to define optimizer and loss function\n",
    "\t\tInputs:\n",
    "\t\t\toutputs - outputs got from decoder part of the network\n",
    "\t\t\ttargets - expected outputs/ labels\n",
    "\t\t\tdec_seq_len -\n",
    "\t\t\tmax_seq_len - \n",
    "\t\t\tlearning_rate - small nubmer used to decrease value of gradients used to update our network\n",
    "\t\t\tclip_rate - tolerance boundries for clipping gradients\n",
    "\t\tOutputs:\n",
    "\t\t\tloss -\n",
    "\t\t\ttrained_opt - optimizer with clipped gradients\n",
    "    '''\n",
    "    logits = tf.identity(outputs.rnn_output)\n",
    "    \n",
    "    mask_weights = tf.sequence_mask(decoder_sequence_length, max_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('opt_loss'):\n",
    "        #using sequence_loss to optimize the seq2seq model\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits, \n",
    "                                                targets, \n",
    "                                                mask_weights)\n",
    "        \n",
    "        #Define optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        #Next 3 lines used to clip gradients {Prevent gradient explosion problem}\n",
    "        gradients = tf.gradients(loss, tf.trainable_variables())\n",
    "        clipped_grads, _ = tf.clip_by_global_norm(gradients, clip_rate)\n",
    "        trained_opt = optimizer.apply_gradients(zip(clipped_grads, tf.trainable_variables()))\n",
    "        \n",
    "    return loss, trained_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adeYZW4j_7-V"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq_Model(object):\n",
    "    \n",
    "    def __init__(self, learning_rate, batch_size, encoder_embedded_size, decoder_embedded_size, rnn_size, \n",
    "                 number_of_layers, vocab_size, word2id_dict, clip_rate):\n",
    "        \n",
    "        #tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs, self.targets, self.keep_prob, self.encoder_sequence_length, self.decoder_sequence_length, max_sequence_length = model_inputs()\n",
    "        \n",
    "        \n",
    "        enc_outputs, enc_states = encoder_rnn(self.inputs, \n",
    "                                          rnn_size,\n",
    "                                          number_of_layers, \n",
    "                                          self.encoder_sequence_length, \n",
    "                                          self.keep_prob, \n",
    "                                          encoder_embedded_size, \n",
    "                                          vocab_size)\n",
    "        \n",
    "        dec_inputs = preprocess_decoder_inputs(self.targets, \n",
    "                                                  word2id_dict, \n",
    "                                                  batch_size)\n",
    "        \n",
    "        \n",
    "        decoder_cell, encoder_states_new = attention_mechanism(rnn_size, \n",
    "                                                          self.keep_prob, \n",
    "                                                          enc_outputs, \n",
    "                                                          enc_states, \n",
    "                                                          self.encoder_sequence_length, \n",
    "                                                          batch_size)\n",
    "        \n",
    "        train_outputs, inference_output = decoder(dec_inputs, \n",
    "                                                  encoder_states_new, \n",
    "                                                  decoder_cell,\n",
    "                                                  decoder_embedded_size, \n",
    "                                                  vocab_size, \n",
    "                                                  self.decoder_sequence_length, \n",
    "                                                  max_sequence_length, \n",
    "                                                  word2id_dict, \n",
    "                                                  batch_size)\n",
    "        \n",
    "        self.predictions  = tf.identity(inference_output.sample_id, name='preds')\n",
    "        \n",
    "        self.loss, self.opt = optimizer_loss(train_outputs, \n",
    "                                       self.targets, \n",
    "                                       self.decoder_sequence_length, \n",
    "                                       max_sequence_length, \n",
    "                                       learning_rate, \n",
    "                                       clip_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-91sT3evJ-k5"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxVs-CJgYLCO"
   },
   "outputs": [],
   "source": [
    "# Fxn to split data itno batches for batch gradient descent                                            \n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "  padded_questions_in_batch = []\n",
    "  padded_answers_in_batch = []\n",
    "  final_question_batches = []\n",
    "  final_answer_batches = []\n",
    "  \n",
    "  for batch_index in range(0,len(questions) // batch_size):\n",
    "      start_index = batch_index * batch_size\n",
    "                                            \n",
    "      questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "      answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "      \n",
    "                         \n",
    "      final_question_batches.append(questions_in_batch)\n",
    "      final_answer_batches.append(answers_in_batch)\n",
    "       \n",
    "  return final_question_batches, final_answer_batches\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3adoQBBzf9V"
   },
   "outputs": [],
   "source": [
    "# Next up: hyper parameters\n",
    "epochs = 10 #100\n",
    "batch_size = 64  #64 make bigger to make faster\n",
    "rnn_size = 64 # 512\n",
    "num_layers = 3  #3\n",
    "encoding_embedding_size = 64 #512  # 512 col in embedding matrix\n",
    "decoding_embedding_size = 64 #512\n",
    "learning_rate = 0.05 # 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.003 #0.0001\n",
    "keep_prob = 0.5\n",
    "keep_probability = 0.5  # based on hinton paper '14'\n",
    "clip= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203602 203602\n"
     ]
    }
   ],
   "source": [
    "# At this point, these MUST be equal\n",
    "print(len(sorted_clean_questions), len(sorted_clean_answers))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckHWyDJgqhdm"
   },
   "source": [
    "# Apply padding to the sequences so the question and answer sequences are the same\n",
    "# THERE IS AN ERROR HERE THAT MAKES A TON OF PADS GET ADDED, CAUSING OOM \n",
    "# Need to force it to 25 to parallel the limits ont he input strings\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZhqWF4lZMZm",
    "scrolled": true
   },
   "source": [
    "# Split data into training and validation sets\n",
    "\n",
    "TTS_TEST_SIZE = 0.2\n",
    "TTS_SEED = 12345\n",
    "\n",
    "train_Q, val_Q, train_A, val_A = \\\n",
    "        train_test_split(sorted_clean_questions, sorted_clean_answers,\n",
    "                     test_size = TTS_TEST_SIZE, random_state = TTS_SEED)\n",
    "\n",
    "padded_train_Q_batches, padded_train_A_batches = \\\n",
    "             split_into_batches(apply_padding(train_Q,questionswords2int),\n",
    "                                apply_padding(train_A,questionswords2int), \n",
    "                                batch_size)\n",
    "\n",
    "#padded_val_Q = apply_padding(val_Q, questionswords2int)\n",
    "#padded_val_A = apply_padding(val_A, questionswords2int)\n",
    "\n",
    "\n",
    "padded_val_Q_batches, padded_val_A_batches = \\\n",
    "            split_into_batches(apply_padding(val_Q, questionswords2int),\n",
    "                              apply_padding(val_A, questionswords2int),\n",
    "                              batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max([len(a) for b in padded_train_A_batches for a in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array([sequence + questionswords2int['<PAD>']] * (25 - len(sequence)) for sequence in sorted_clean_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.asarray(padded_train_A_batches)[0].shape)\n",
    "#print(np.asarray(padded_train_A_batches)[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Onesuch error is (299, 20, 69xxx) ... 20 batches of 299 \"words\" of 69xxx possible words\n",
    "print(np.asarray(padded_val_Q_batches).shape)\n",
    "print(np.asarray(padded_val_A_batches).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoddAR_lpdl3"
   },
   "outputs": [],
   "source": [
    "# If needed during testing\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTvlebyhA0YC"
   },
   "outputs": [],
   "source": [
    "# instantiate the Seq2Seq model using graph1\n",
    "# starts with resetting graph1 for debugging purposes\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph1 = tf.get_default_graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "  \n",
    "  model = Seq2Seq_Model(learning_rate, batch_size, encoding_embedding_size, \n",
    "                        decoding_embedding_size,\n",
    "                        rnn_size, num_layers, len(word2count), \n",
    "                        questionswords2int, clip)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_Q_batches = padded_train_Q_batches[0:999]\n",
    "padded_train_A_batches = padded_train_Q_batches[0:999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-ZW14YCjxNt"
   },
   "outputs": [],
   "source": [
    "# establish session using graph1\n",
    "\n",
    "session = tf.Session(graph = graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "c3dGVvipCPV9",
    "outputId": "69779376-bec4-473b-db4a-6519ff42483e"
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "# initialize global variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "PRINT_ERROR = True\n",
    "\n",
    "# empty lists\n",
    "epoch_accuracy = []\n",
    "epoch_loss = []\n",
    "\n",
    "# Start training loop\n",
    "for i in range(epochs):\n",
    "  \n",
    "  # empty lists to collect loss and acc for bucketd training sets\n",
    "  batch_accuracy = []\n",
    "  batch_loss = []\n",
    "  \n",
    "  # for bucket index\n",
    "  # tqdm is a progress bar that does not look good in notebooks\n",
    "  # but looks good in command line\n",
    "  #for b_idx in tqdm(len(padded_train_Q_batches)): ### commented out for debugging\n",
    "  \n",
    "  for b_idx in range(len(padded_train_A_batches)): #<-- for debugging\n",
    "  \n",
    "    # convert the python arrays to numpy arrays\n",
    "    X_batch = np.asarray(padded_train_Q_batches[b_idx])\n",
    "    y_batch = np.asarray(padded_train_A_batches[b_idx])\n",
    "    ### print(X_batch.shape, y_batch.shape)\n",
    "\n",
    "    # create the feed_dict for the model creation steps\n",
    "    feed_dict = {model.inputs:X_batch, \n",
    "         model.targets:y_batch, \n",
    "         model.keep_prob:keep_prob, \n",
    "         model.decoder_sequence_length:[len(y_batch[0])]*batch_size,\n",
    "         model.encoder_sequence_length:[len(X_batch[0])]*batch_size}\n",
    "\n",
    "    \n",
    "    \n",
    "    # a single step of batch gradient descent\n",
    "    cost, _, preds = session.run([model.loss, model.opt, model.predictions], feed_dict=feed_dict, options = run_opts)\n",
    "\n",
    "    # collect loss/acc for each batch\n",
    "    batch_loss.append(cost)\n",
    "    batch_accuracy.append(get_accuracy(y_batch, preds))\n",
    "\n",
    "       \n",
    "    #if(PRINT_ERROR == True and b_idx%100 == 0): \n",
    "    if(PRINT_ERROR == True and b_idx%5 == 0):\n",
    "      print(\" Bucket {}:\".format(b_idx), \n",
    "          \" | Loss: {}\".format(np.mean(batch_loss)), \n",
    "          \" | Accuracy: {}\".format(np.mean(batch_accuracy)))\n",
    "\n",
    "  epoch_loss.append(np.mean(batch_loss))\n",
    "  epoch_accuracy.append(np.mean(batch_accuracy))\n",
    "  \n",
    "  # Print epoch and CV loss/accuracy:\n",
    "  #if(PRINT_ERROR == True and i%100 == 0):\n",
    "  if(PRINT_ERROR == True and i%1 == 0):\n",
    "      val_losses = []\n",
    "      val_acc = []\n",
    "      \n",
    "      for v_idx in range(2):#len(padded_val_Q_batches)):\n",
    "        X_val = np.asarray(padded_val_Q_batches[v_idx])\n",
    "        \n",
    "        y_val = np.asarray(padded_val_A_batches[v_idx])\n",
    "        ### print(X_val.shape, y_val.shape)\n",
    "        \n",
    "        # validation feed_dict\n",
    "        val_feed_dict = {model.inputs:X_val, \n",
    "                         model.targets:y_val, \n",
    "                         model.keep_prob:1, \n",
    "                         model.decoder_sequence_length:[len(y_val[0])]*batch_size,\n",
    "                         model.encoder_sequence_length:[len(X_val[0])]*batch_size}\n",
    "\n",
    "        \n",
    "        # run model loss and predictions, but not optimization -- scoring, not training!\n",
    "        val_loss, val_preds = session.run([model.loss, model.predictions], feed_dict = val_feed_dict)\n",
    "                \n",
    "        val_losses.append(val_loss)\n",
    "        val_acc.append(get_accuracy(y_val, val_preds))\n",
    "        #val_acc.append(tf.metrics.accuracy(y_val, val_preds)) ##<-- causes attribute error in np.mean()\n",
    "           \n",
    "      print(\"EPOCH[{}]: {}/{}\".format(i, i+1, epochs), \n",
    "          \"\\n --->| loss: {} val: {}\".format(np.mean(epoch_loss), np.mean(val_losses)), \n",
    "          \"\\n --->| acc: {} val: {}\".format(np.mean(epoch_accuracy), np.mean(val_acc)))\n",
    "\n",
    "  saver.save(session, \"checkpoint/chatbot_{}.ckpt\".format(i))\n",
    "    \n",
    "session.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Debugging Parameters\n",
    "# Q's | BS | bmod | epochs\n",
    "# OK: 50 | 10 | %2 | 5 \n",
    "# NOT OK: 100 | 20 | %2 | 5\n",
    "# error: OOM [299, 20, 69k] <--- an answer. Why is that 299? 2GB\n",
    "#\n",
    "# after fixed bug, should not have OOM error now\n",
    "# 1000 | 64 | %5 | 10\n",
    "# WORKS GREAT! (stopped early by force)\n",
    "#EPOCH[5]: 6/10 \n",
    "#--->| loss: 1.7298284769058228 val: 5.566536903381348 \n",
    "#--->| acc: 0.693757924591258 val: 0.49968749999999995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USER ASKS, BOT ANSWERS\n",
    "#### TODO: \n",
    "- Prep model for single input prediction\n",
    "- Hacked\n",
    "- Train a model more deeply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "\n",
    "    # be wary of relationship of array dimensions between this single string and the\n",
    "    # batch of strings being used to train the model, mismatch will need to be\n",
    "    # compensated for\n",
    "    user_input = input()\n",
    "\n",
    "    cleaned_user_input = clean_text(user_input)\n",
    "    cleaned_user_input += ' <EOS>'\n",
    "    clean_len = len(cleaned_user_input.split())\n",
    "    \n",
    "    MAX_SENTENCE_LENGTH = 25\n",
    "    for i in range(0,MAX_SENTENCE_LENGTH):\n",
    "        if i >= clean_len:\n",
    "            #print(i, MAX_SENTENCE_LENGTH)\n",
    "            cleaned_user_input += ' <PAD>'\n",
    "\n",
    "    cleaned_user_input_list = cleaned_user_input.split()\n",
    "    user_input_ints = word_into_int(cleaned_user_input_list, questionswords2int)\n",
    "\n",
    "    # hack b/c only using 1 line.\n",
    "    user_ints = [i[0] for i in user_input_ints]\n",
    "    return user_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here if needed during debugging\n",
    "# session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph1 = tf.get_default_graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "\n",
    "    model = Seq2Seq_Model(learning_rate, batch_size, encoding_embedding_size, \n",
    "                        decoding_embedding_size,\n",
    "                        rnn_size, num_layers, len(word2count), \n",
    "                        questionswords2int, clip)\n",
    "\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very rough putting together of the above pieces\n",
    "\n",
    "def get_user_output(batch_size, answerswords2int):\n",
    "    \n",
    "    user_ints = get_user_input()\n",
    "    #print(user_ints)\n",
    "    \n",
    "    while user_ints != 'quit':\n",
    "\n",
    "        response = ''\n",
    "\n",
    "\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            saver.restore(session, save_path='/PYTHON_VIRTUAL_ENVIRONMENTS/Chatbots/checkpoint/chatbot_5.ckpt')\n",
    "\n",
    "            # not sure how to deliver a single entry for predictions\n",
    "            # keep getting dim error, so I am putting batch size repetition of the user input\n",
    "            # very hacky\n",
    "            tiled_input = np.asarray([user_ints for i in range(batch_size)])\n",
    "\n",
    "            feed_dict = {model.inputs:tiled_input,\n",
    "                         model.targets:np.zeros(tiled_input.shape), \n",
    "                         model.keep_prob:1, \n",
    "                         model.decoder_sequence_length:[len(user_ints)]*batch_size,\n",
    "                         model.encoder_sequence_length:[len(user_ints)]*batch_size}\n",
    "\n",
    "            pred = session.run([model.predictions], feed_dict)\n",
    "            #print(type(pred))\n",
    "\n",
    "            invert_dict = inv_map = {v: k for k, v in answerswords2int.items()}\n",
    "\n",
    "            text_pred = [invert_dict[p] for p in pred[0][0]]\n",
    "\n",
    "            #print(text_pred)\n",
    "            #print('len: {}'.format(len(text_pred)))\n",
    "\n",
    "            text_pred = [w for w in text_pred if not w == '<PAD>']\n",
    "            response = ' '.join(text_pred)\n",
    "            #construct sentence\n",
    "            print(response)\n",
    "            \n",
    "        user_ints = get_user_input()\n",
    "\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chad, I am testing you. What do you think about that?\n",
      "INFO:tensorflow:Restoring parameters from /PYTHON_VIRTUAL_ENVIRONMENTS/Chatbots/checkpoint/chatbot_5.ckpt\n",
      "<OUT> i am you to you are you to you think that\n"
     ]
    }
   ],
   "source": [
    "# Really slow. Need to implement the model loading properly.\n",
    "get_user_output(batch_size, answerswords2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# instantiate the Seq2Seq model using graph1\n",
    "# starts with resetting graph1 for debugging purposes\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph1 = tf.get_default_graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "  \n",
    "  model = Seq2Seq_Model(learning_rate, batch_size, encoding_embedding_size, \n",
    "                        decoding_embedding_size,\n",
    "                        rnn_size, num_layers, len(word2count), \n",
    "                        questionswords2int, clip)\n",
    "  \n",
    "saver = tf.train.Saver()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tiled_input = np.asarray([user_ints for i in range(batch_size)])\n",
    "print(tiled_input)\n",
    "print(tiled_input.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with tf.Session() as session:\\n    saver.restore(session, save_path='/PYTHON_VIRTUAL_ENVIRONMENTS/Chatbots/checkpoint/chatbot_5.ckpt')\\n    \\n    # not sure how to deliver a single entry for predictions\\n    # keep getting dim error, so I am putting batch size repetition of the user input\\n    # very hacky\\n    tiled_input = np.asarray([user_ints for i in range(batch_size)])\\n    \\n    feed_dict = {model.inputs:tiled_input,\\n                 model.targets:np.zeros(tiled_input.shape), \\n                 model.keep_prob:1, \\n                 model.decoder_sequence_length:[len(user_ints)]*batch_size,\\n                 model.encoder_sequence_length:[len(user_ints)]*batch_size}\\n    \\n    pred = session.run([model.predictions], feed_dict)\\n    #print(type(pred))\\n    \\n    text_pred = [map_invert_answers_to_ints[i] for i in pred[0][0]]\\n    \\n    print(text_pred)\\n    print('len: {}'.format(len(text_pred)))\\n    \\n    text_pred = [w for w in text_pred if not w == '<PAD>']\\n    response = ' '.join(text_pred)\\n    #construct sentence\\n    print(response)\\n    \\n    \\n    \\n    \\n\\n# instantiate the Seq2Seq model using graph1\\n# starts with resetting graph1 for debugging purposes\\n    \\n    \\n    \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with tf.Session() as session:\n",
    "    saver.restore(session, save_path='/PYTHON_VIRTUAL_ENVIRONMENTS/Chatbots/checkpoint/chatbot_5.ckpt')\n",
    "    \n",
    "    # not sure how to deliver a single entry for predictions\n",
    "    # keep getting dim error, so I am putting batch size repetition of the user input\n",
    "    # very hacky\n",
    "    tiled_input = np.asarray([user_ints for i in range(batch_size)])\n",
    "    \n",
    "    feed_dict = {model.inputs:tiled_input,\n",
    "                 model.targets:np.zeros(tiled_input.shape), \n",
    "                 model.keep_prob:1, \n",
    "                 model.decoder_sequence_length:[len(user_ints)]*batch_size,\n",
    "                 model.encoder_sequence_length:[len(user_ints)]*batch_size}\n",
    "    \n",
    "    pred = session.run([model.predictions], feed_dict)\n",
    "    #print(type(pred))\n",
    "    \n",
    "    text_pred = [map_invert_answers_to_ints[i] for i in pred[0][0]]\n",
    "    \n",
    "    print(text_pred)\n",
    "    print('len: {}'.format(len(text_pred)))\n",
    "    \n",
    "    text_pred = [w for w in text_pred if not w == '<PAD>']\n",
    "    response = ' '.join(text_pred)\n",
    "    #construct sentence\n",
    "    print(response)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# instantiate the Seq2Seq model using graph1\n",
    "# starts with resetting graph1 for debugging purposes\n",
    "    \n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Seq2Seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tf19_chatbot",
   "language": "python",
   "name": "tf19_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
